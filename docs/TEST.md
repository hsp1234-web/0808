> **[Jules's Note]**
> 本文件描述的是專案早期 V34 架構下的測試策略，該策略圍繞一個統一的後端服務 (`phoenix_core`) 進行。此模型已被後續的「預烘烤環境」架構取代，其中的工具都是獨立運行的。因此，本文件中描述的測試腳本 (`test_full_system_flow.py` 等) 已不再適用。本文件僅供歷史參考。

---

# 測試策略與執行指南 (V34)

**文件作者：** Jules (AI 軟體工程師)
**最後更新：** 2025年8月7日

## 一、 核心理念：分層的端對端驗證

隨著專案架構的成熟，我們的測試策略已演進為一個分層、職責分明的端對端（E2E）測試模型。我們現在擁有兩層核心的 E2E 測試：

### **第一層：後端邏輯驗證 (Backend Logic Verification)**
- **核心腳本：** `tests/e2e/test_full_system_flow.py`
- **設計理念：** 在不啟動真實瀏覽器的情況下，透過程式碼精準地模擬核心業務邏輯，並驗證整個後端服務的反應是否符合預期。
- **測試重點：** 這個測試專注於回答以下關鍵問題：
1.  模擬 `@markdown` 參數生成的設定檔，能否被後端服務正確解析？
2.  後端 API 服務能否在一個乾淨、隔離的虛擬環境中成功啟動？
3.  API 端點（日誌、效能）的回傳值是否正確？
4.  報告生成功能是否能在任務結束後，根據資料庫狀態成功產出報告？
5.  `git clone` 下載功能是否正常？
6.  整個流程是否能在設定的超時時間內完成，以防止 CI/CD 流程卡死？

### **第二層：使用者介面互動驗證 (UI Interaction Verification)**
- **核心腳本：** `tests/e2e/test_ui_interactions.py`
- **設計理念：** 使用真實的瀏覽器環境，模擬使用者在 `wolf.html` 儀表板上的實際操作（如點擊、切換分頁），並驗證前端與後端之間的互動是否如預期般被記錄下來。
- **測試重點：** 這個測試專注於回答以下問題：
    1.  後端服務是否能成功地提供 `wolf.html` 前端頁面？
    2.  Playwright 測試框架能否成功驅動瀏覽器並與頁面上的元件互動？
    3.  當使用者點擊按鈕或切換分頁時，前端發出的 API 請求是否能被後端正確接收？
    4.  這些互動事件是否成功地、以正確的格式被記錄在後端日誌中？

---

## 二、 如何執行測試

根據不同的測試目標，使用對應的命令。

### **執行後端邏輯測試:**

在專案的根目錄下，使用 `pytest` 執行 `test_full_system_flow.py` 腳本：

### **前置要求：**

- **Python 3.8+ 環境**
- **Git** 已安裝並在系統 PATH 中
- **虛擬環境與依賴**:
  - 需先建立虛擬環境 (`uv venv`)
  - 安裝核心依賴 (`uv pip install -r requirements/base.txt`)
  - 安裝開發依賴 (`uv pip install -r requirements/dev.txt`)
  - 以可編輯模式安裝專案 (`uv pip install -e .`)

### **執行測試套件:**

在專案的根目錄下，使用 `pytest` 執行 `test_full_system_flow.py` 腳本：

```bash
# 推薦的執行方式 (需要先設定好環境)
.venv/bin/python -m pytest -v tests/e2e/test_full_system_flow.py
```

### **執行 UI 互動測試:**

此測試需要 `pytest-playwright` 套件。請確保已安裝開發依賴。

```bash
# 推薦的執行方式 (需要先設定好環境)
.venv/bin/python -m pytest -v tests/e2e/test_ui_interactions.py
```
此測試會啟動後端伺服器，打開真實的瀏覽器進行點擊操作，然後關閉伺服器。測試日誌會保存在 `server_e2e_test.log` 中，**檢查此檔案中的 `[UI_EVENT_LOGGER]` 記錄是驗證此測試是否成功的關鍵步驟**。

---

## 三、 核心測試詳解

### **`test_full_system_flow.py` (後端邏輯)**

這是我們目前最重要的測試檔案，它是一個精心設計的自動化流程，包含以下幾個階段：

### **階段一：自我引導與環境準備 (Bootstrap)**
-   **目的**: 解決在任何機器上（開發者本機、CI 伺服器）的環境不一致問題。
-   **作法**:
    1.  腳本啟動時，首先檢查自己是否在專案的 `.venv` 虛擬環境中執行。
    2.  如果**不是**，它會自動建立 `.venv`，並安裝 `requirements/` 目錄中定義的所有依賴（包括 `base`, `dev`, `report`）。
    3.  安裝完畢後，它會使用 `.venv/bin/python` **重新啟動自己**。
    4.  如果**是**，則直接進入下一階段。
-   **優勢**: 此設計確保了後續的所有測試邏輯，都運行在一個具備所有正確依賴的、乾淨的環境中。

### **階段二：獨立功能驗證**
在啟動完整的伺服器流程前，腳本會先對幾個可以獨立驗證的功能進行快速測試。
-   **`run_download_test()`**:
    -   **目標**: 驗證 `colab_runner.py` 中定義的 `git clone` 功能。
    -   **作法**: 在一個臨時目錄中執行 `git clone`，檢查關鍵檔案是否存在，然後**無論成功或失敗，都立即刪除該臨時目錄**。
    -   **優勢**: 測試了下載功能，同時保證了零檔案殘留，極度乾淨。

### **階段三：完整流程模擬**
這是測試的核心，它在一個迴圈中執行所有測試案例（目前整合為一個 `full_flow_test`）。
1.  **啟動伺服器 (`start_server`)**:
    -   模擬 `colab_runner.py` 的行為，為本次測試動態生成一個 `config.json`。
    -   使用 `subprocess.Popen` 在背景啟動 `scripts/start_api_service.py`。
    -   **看門狗 (Watchdog)**: 同時啟動一個 10 秒的定時器。如果 10 秒內沒有收到任何日誌輸出，測試將被強制中止並標記為失敗。
2.  **API 驗證 (`run_api_tests`)**:
    -   驗證根目錄 `/` 是否可訪問。
    -   驗證 `/api/v1/status/dashboard` 回傳的日誌是否符合 `config.json` 的過濾規則。
3.  **刷新率驗證 (`run_refresh_rate_test`)**:
    -   連續請求 `/api/v1/status/performance` 兩次，驗證回傳的系統資源數據是即時變動的。
4.  **報告生成驗證 (`run_report_generation_test`)**:
    -   **偽造資料庫**: 由於後端 API 目前是 Mock 狀態，此函式會先**動態建立一個假的 `state.db`**，並填入模擬的日誌和狀態資料。
    -   **執行與驗證**: 接著執行 `scripts/generate_report.py`，並斷言所有預期的 Markdown 報告是否都已生成。
5.  **清理 (`cleanup`)**:
    -   在每個測試案例的 `finally` 區塊中被呼叫。
    -   負責終止伺服器進程、刪除偽造的資料庫、刪除生成的報告目錄和臨時的設定檔。

這套測試完整地驗證了專案所有核心功能的正確性，是我們確保程式碼品質的基石。

### **`test_ui_interactions.py` (UI 互動)**

這個測試腳本是 V34 版新增的，其核心職責是驗證前端與後端的連結性。

1.  **伺服器 Fixture (`live_server`)**:
    -   **目標**: 在一個真實的環境中測試前端頁面。
    -   **作法**: 使用 `pytest` 的 `fixture`，在測試開始前，透過 `subprocess.Popen` 在背景啟動一個真實的 Uvicorn 伺服器。所有伺服器日誌都會被重定向到 `server_e2e_test.log`。測試結束後，`fixture` 會負責終止伺服器進程。

2.  **互動與日誌生成**:
    -   **目標**: 模擬使用者點擊，觸發後端日誌。
    -   **作法**:
        1.  使用 Playwright 前往 `wolf.html` 主頁面。
        2.  定義一個包含所有關鍵互動元件（分頁、按鈕）的列表。
        3.  依正確的邏輯順序（先切換分頁，再點擊內部元件）遍歷列表，模擬點擊操作。
        4.  此測試**不包含任何 UI 狀態的斷言 (assert)**。它的唯一目的是觸發前端的 `logUIEvent` 函式，向後端發送日誌請求。

3.  **除錯紀要 (Lessons Learned)**:
    -   在開發此測試的過程中，我們解決了幾個關鍵問題，這些經驗對於未來開發很有價值：
        -   **問題一：API 404 Not Found**
            -   **現象**: 前端發出的日誌請求全部失敗。
            -   **原因**: API 路由邏輯被錯誤地放在 `src/phoenix_core/api/` 目錄下，而該目錄不會被 `main.py` 的動態模組加載機制掃描。
            -   **解決方案**: 將路由邏輯移至會被掃描的 `src/phoenix_core/modules/system_actions/__init__.py` 中，問題解決。
        -   **問題二：API 500 Internal Server Error**
            -   **現象**: API 請求雖然成功送達，但伺服器返回內部錯誤。
            -   **原因**: 呼叫日誌函式 `logger.log()` 時，傳入了它不接受的自訂關鍵字參數 (`component_id` 等)。
            -   **解決方案**: 修改呼叫方式，將所有額外資訊格式化到 `message` 字串中，並善用 `source` 參數來標記事件來源，符合了 `logger.py` 的設計。
        -   **問題三：Playwright 元素不可見 (Element is not visible)**
            -   **現象**: 測試在點擊某個按鈕時因超時而失敗。
            -   **原因**: 測試腳本的操作順序錯誤，在點擊一個位於隱藏分頁內的按鈕前，沒有先點擊分頁本身使其可見。
            -   **解決方案**: 調整測試腳本中的操作順序，確保與真實使用者的操作邏輯一致。
