# api_server.py
import uuid
import shutil
import logging
import json
import subprocess
import sys
import threading
import asyncio
import os
import time
from fastapi import FastAPI, UploadFile, File, Form, Request, HTTPException, WebSocket, WebSocketDisconnect, Query
from fastapi.responses import HTMLResponse, JSONResponse
from fastapi.staticfiles import StaticFiles
from fastapi.middleware.cors import CORSMiddleware
from pathlib import Path
from typing import Optional, Dict, List

# åŒ¯å…¥æ–°çš„è³‡æ–™åº«å®¢æˆ¶ç«¯
# from db import database # REMOVED: No longer used directly
from db.client import get_client

# --- JULES æ–¼ 2025-08-09 çš„ä¿®æ”¹ï¼šè¨­å®šæ‡‰ç”¨ç¨‹å¼å…¨åŸŸæ™‚å€ ---
# ç‚ºäº†ç¢ºä¿æ‰€æœ‰æ—¥èªŒå’Œè³‡æ–™åº«æ™‚é–“æˆ³éƒ½ä½¿ç”¨ä¸€è‡´çš„æ™‚å€ï¼Œæˆ‘å€‘åœ¨æ‡‰ç”¨ç¨‹å¼å•Ÿå‹•çš„
# æœ€æ—©æœŸéšæ®µå°±å°‡æ™‚å€ç’°å¢ƒè®Šæ•¸è¨­å®šç‚º 'Asia/Taipei'ã€‚
os.environ['TZ'] = 'Asia/Taipei'
if sys.platform != 'win32':
    time.tzset()
# --- æ™‚å€è¨­å®šçµæŸ ---

# --- æ¨¡å¼è¨­å®š ---
# JULES: æ”¹ç‚ºé€éç’°å¢ƒè®Šæ•¸ä¾†æ±ºå®šæ¨¡æ“¬æ¨¡å¼ï¼Œä»¥ä¾¿èˆ‡ Circus æ•´åˆ
# é è¨­ç‚ºéæ¨¡æ“¬æ¨¡å¼ (çœŸå¯¦æ¨¡å¼)
IS_MOCK_MODE = os.environ.get("API_MODE", "real") == "mock"

# --- è·¯å¾‘è¨­å®š ---
# ä»¥æ­¤æª”æ¡ˆç‚ºåŸºæº–ï¼Œå®šç¾©å°ˆæ¡ˆæ ¹ç›®éŒ„
ROOT_DIR = Path(__file__).resolve().parent

# --- ä¸»æ—¥èªŒè¨­å®š ---
# ä¸»æ—¥èªŒå™¨
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()] # è¼¸å‡ºåˆ°æ§åˆ¶å°
)
log = logging.getLogger('api_server')

def setup_database_logging():
    """è¨­å®šè³‡æ–™åº«æ—¥èªŒè™•ç†å™¨ã€‚"""
    try:
        from db.log_handler import DatabaseLogHandler
        root_logger = logging.getLogger()
        # æª¢æŸ¥æ˜¯å¦å·²ç¶“æœ‰åŒé¡å‹çš„ handlerï¼Œé¿å…é‡è¤‡åŠ å…¥
        if not any(isinstance(h, DatabaseLogHandler) for h in root_logger.handlers):
            root_logger.addHandler(DatabaseLogHandler(source='api_server'))
            log.info("è³‡æ–™åº«æ—¥èªŒè™•ç†å™¨è¨­å®šå®Œæˆ (source: api_server)ã€‚")
    except Exception as e:
        log.error(f"æ•´åˆè³‡æ–™åº«æ—¥èªŒæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)


# Frontend action logging is now handled by the centralized database logger.


# --- WebSocket é€£ç·šç®¡ç†å™¨ ---
class ConnectionManager:
    def __init__(self):
        self.active_connections: list[WebSocket] = []

    async def connect(self, websocket: WebSocket):
        await websocket.accept()
        self.active_connections.append(websocket)
        log.info(f"æ–°ç”¨æˆ¶ç«¯é€£ç·šã€‚ç›®å‰å…± {len(self.active_connections)} å€‹é€£ç·šã€‚")

    def disconnect(self, websocket: WebSocket):
        self.active_connections.remove(websocket)
        log.info(f"ä¸€å€‹ç”¨æˆ¶ç«¯é›¢ç·šã€‚ç›®å‰å…± {len(self.active_connections)} å€‹é€£ç·šã€‚")

    async def send_personal_message(self, message: str, websocket: WebSocket):
        await websocket.send_text(message)

    async def broadcast(self, message: str):
        for connection in self.active_connections:
            await connection.send_text(message)

    async def broadcast_json(self, data: dict):
        for connection in self.active_connections:
            await connection.send_json(data)

manager = ConnectionManager()


from contextlib import asynccontextmanager

# --- DB å®¢æˆ¶ç«¯ ---
# åœ¨æ¨¡çµ„åŠ è¼‰æ™‚ç²å–å®¢æˆ¶ç«¯å–®ä¾‹
# å®¢æˆ¶ç«¯å…§éƒ¨æœ‰é‡è©¦æ©Ÿåˆ¶ï¼Œæœƒç­‰å¾… DB ç®¡ç†è€…æœå‹™å°±ç·’
db_client = get_client()

# --- FastAPI Lifespan Manager ---
@asynccontextmanager
async def lifespan(app: FastAPI):
    # åœ¨æ‡‰ç”¨ç¨‹å¼å•Ÿå‹•æ™‚åŸ·è¡Œçš„ç¨‹å¼ç¢¼
    setup_database_logging()
    log.info("è³‡æ–™åº«æ—¥èªŒè™•ç†å™¨å·²é€é lifespan äº‹ä»¶è¨­å®šã€‚")
    yield
    # å¯ä»¥åœ¨æ­¤è™•åŠ å…¥æ‡‰ç”¨ç¨‹å¼é—œé–‰æ™‚åŸ·è¡Œçš„ç¨‹å¼ç¢¼

# --- FastAPI æ‡‰ç”¨å¯¦ä¾‹ ---
app = FastAPI(title="é³³å‡°éŸ³è¨Šè½‰éŒ„å„€ API (v3 - é‡æ§‹)", version="3.0", lifespan=lifespan)

# --- ä¸­ä»‹è»Ÿé«” (Middleware) ---
# JULES: æ–°å¢ CORS ä¸­ä»‹è»Ÿé«”ä»¥å…è¨±ä¾†è‡ªç€è¦½å™¨è…³æœ¬çš„è·¨ä¾†æºè«‹æ±‚
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # å…è¨±æ‰€æœ‰ä¾†æº
    allow_credentials=True,
    allow_methods=["*"],  # å…è¨±æ‰€æœ‰æ–¹æ³•
    allow_headers=["*"],  # å…è¨±æ‰€æœ‰æ¨™é ­
)

# --- è·¯å¾‘è¨­å®š ---
# ä»¥æ­¤æª”æ¡ˆç‚ºåŸºæº–ï¼Œå®šç¾©å°ˆæ¡ˆæ ¹ç›®éŒ„
ROOT_DIR = Path(__file__).resolve().parent
# æ–°çš„ä¸Šå‚³æª”æ¡ˆå„²å­˜ç›®éŒ„
UPLOADS_DIR = ROOT_DIR / "uploads"
# éœæ…‹æª”æ¡ˆç›®éŒ„
STATIC_DIR = ROOT_DIR / "static"

# ç¢ºä¿ç›®éŒ„å­˜åœ¨
UPLOADS_DIR.mkdir(exist_ok=True)
if not STATIC_DIR.exists():
    log.warning(f"éœæ…‹æª”æ¡ˆç›®éŒ„ {STATIC_DIR} ä¸å­˜åœ¨ï¼Œå‰ç«¯é é¢å¯èƒ½ç„¡æ³•è¼‰å…¥ã€‚")
else:
    app.mount("/static", StaticFiles(directory=STATIC_DIR), name="static")


# --- API ç«¯é» ---

@app.get("/", response_class=HTMLResponse)
async def serve_frontend(request: Request):
    """æ ¹ç«¯é»ï¼Œæä¾›å‰ç«¯æ“ä½œä»‹é¢ã€‚"""
    html_file_path = STATIC_DIR / "mp3.html"
    if not html_file_path.is_file():
        log.error(f"æ‰¾ä¸åˆ°å‰ç«¯æª”æ¡ˆ: {html_file_path}")
        raise HTTPException(status_code=404, detail="æ‰¾ä¸åˆ°å‰ç«¯ä»‹é¢æª”æ¡ˆ (mp3.html)")
    return HTMLResponse(content=html_file_path.read_text(encoding="utf-8"), status_code=200)


def check_model_exists(model_size: str) -> bool:
    """
    æª¢æŸ¥æŒ‡å®šçš„ Whisper æ¨¡å‹æ˜¯å¦å·²ç¶“è¢«ä¸‹è¼‰åˆ°æœ¬åœ°å¿«å–ã€‚
    """
    # JULES'S FIX: å¢åŠ ä¸€å€‹ç’°å¢ƒè®Šæ•¸ä¾†å¼·åˆ¶ä½¿ç”¨æ¨¡æ“¬è½‰éŒ„å™¨ï¼Œä»¥æ”¯æ´æ··åˆæ¨¡å¼æ¸¬è©¦
    force_mock = os.environ.get("FORCE_MOCK_TRANSCRIBER") == "true"
    tool_script = "tools/mock_transcriber.py" if IS_MOCK_MODE or force_mock else "tools/transcriber.py"
    log.info(f"ä½¿ç”¨ '{tool_script}' æª¢æŸ¥æ¨¡å‹ '{model_size}' æ˜¯å¦å­˜åœ¨...")

    # æˆ‘å€‘é€éå‘¼å«ä¸€å€‹è¼•é‡ç´šçš„å·¥å…·è…³æœ¬ä¾†æª¢æŸ¥ã€‚
    check_command = [sys.executable, tool_script, "--command=check", f"--model_size={model_size}"]
    try:
        # åœ¨æ¨¡æ“¬æ¨¡å¼ä¸‹ï¼Œmock_transcriber.py æœƒæ°¸é å›å‚³ "exists"
        result = subprocess.run(check_command, capture_output=True, text=True, check=True)
        output = result.stdout.strip().lower()
        log.info(f"æ¨¡å‹ '{model_size}' æª¢æŸ¥çµæœ: {output}")
        # å¿…é ˆå®Œå…¨åŒ¹é… "exists"ï¼Œé¿å… "not_exists" è¢«éŒ¯èª¤åˆ¤æ–·ç‚º True
        return output == "exists"
    except (subprocess.CalledProcessError, FileNotFoundError) as e:
        log.error(f"æª¢æŸ¥æ¨¡å‹ '{model_size}' æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return False

@app.post("/api/transcribe", status_code=202)
async def create_transcription_task(
    file: UploadFile = File(...),
    model_size: str = Form("tiny"),
    language: Optional[str] = Form(None),
    beam_size: int = Form(5)
):
    """
    æ¥æ”¶éŸ³è¨Šæª”æ¡ˆï¼Œæ ¹æ“šæ¨¡å‹æ˜¯å¦å­˜åœ¨ï¼Œæ±ºå®šæ˜¯ç›´æ¥å»ºç«‹è½‰éŒ„ä»»å‹™ï¼Œ
    é‚„æ˜¯å…ˆå»ºç«‹ä¸€å€‹ä¸‹è¼‰ä»»å‹™å’Œä¸€å€‹ä¾è³´æ–¼å®ƒçš„è½‰éŒ„ä»»å‹™ã€‚
    """
    # 1. æª¢æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
    model_is_present = check_model_exists(model_size)

    # 2. ä¿å­˜ä¸Šå‚³çš„æª”æ¡ˆ
    transcribe_task_id = str(uuid.uuid4())
    file_extension = Path(file.filename).suffix or ".wav"
    saved_file_path = UPLOADS_DIR / f"{transcribe_task_id}{file_extension}"
    try:
        with open(saved_file_path, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
        log.info(f"æª”æ¡ˆå·²å„²å­˜è‡³: {saved_file_path}")
    except Exception as e:
        log.error(f"âŒ å„²å­˜æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"ç„¡æ³•å„²å­˜ä¸Šå‚³çš„æª”æ¡ˆ: {e}")
    finally:
        await file.close()

    # 3. æ ¹æ“šæ¨¡å‹æ˜¯å¦å­˜åœ¨ä¾†å»ºç«‹ä»»å‹™
    transcription_payload = {
        "input_file": str(saved_file_path),
        "original_filename": file.filename, # JULES'S FIX: Store the original filename
        "output_dir": "transcripts",
        "model_size": model_size,
        "language": language,
        "beam_size": beam_size
    }

    if model_is_present:
        # æ¨¡å‹å·²å­˜åœ¨ï¼Œç›´æ¥å»ºç«‹è½‰éŒ„ä»»å‹™
        log.info(f"âœ… æ¨¡å‹ '{model_size}' å·²å­˜åœ¨ï¼Œç›´æ¥å»ºç«‹è½‰éŒ„ä»»å‹™: {transcribe_task_id}")
        db_client.add_task(transcribe_task_id, json.dumps(transcription_payload), task_type='transcribe')
        # JULES: ä¿®æ­£ API å›æ‡‰ï¼Œä½¿å…¶èˆ‡å‰ç«¯çš„é€šç”¨è™•ç†é‚è¼¯ä¸€è‡´ï¼Œè£œä¸Š type æ¬„ä½
        return {"task_id": transcribe_task_id, "type": "transcribe"}
    else:
        # æ¨¡å‹ä¸å­˜åœ¨ï¼Œå»ºç«‹ä¸‹è¼‰ä»»å‹™å’Œä¾è³´çš„è½‰éŒ„ä»»å‹™
        download_task_id = str(uuid.uuid4())
        log.warning(f"âš ï¸ æ¨¡å‹ '{model_size}' ä¸å­˜åœ¨ã€‚å»ºç«‹ä¸‹è¼‰ä»»å‹™ '{download_task_id}' å’Œä¾è³´çš„è½‰éŒ„ä»»å‹™ '{transcribe_task_id}'")

        download_payload = {"model_size": model_size}
        db_client.add_task(download_task_id, json.dumps(download_payload), task_type='download')

        db_client.add_task(transcribe_task_id, json.dumps(transcription_payload), task_type='transcribe', depends_on=download_task_id)

        # æˆ‘å€‘å›å‚³è½‰éŒ„ä»»å‹™çš„ IDï¼Œè®“å‰ç«¯å¯ä»¥è¿½è¹¤æœ€çµ‚çµæœ
        return JSONResponse(content={"tasks": [
            {"task_id": download_task_id, "type": "download"},
            {"task_id": transcribe_task_id, "type": "transcribe"}
        ]})


@app.get("/api/status/{task_id}")
async def get_task_status_endpoint(task_id: str):
    """
    æ ¹æ“šä»»å‹™ IDï¼Œå¾è³‡æ–™åº«æŸ¥è©¢ä»»å‹™ç‹€æ…‹ã€‚
    """
    log.debug(f"ğŸ” æ­£åœ¨æŸ¥è©¢ä»»å‹™ç‹€æ…‹: {task_id}")
    status_info = db_client.get_task_status(task_id)

    if not status_info:
        log.warning(f"â“ æ‰¾ä¸åˆ°ä»»å‹™ ID: {task_id}")
        raise HTTPException(status_code=404, detail="æ‰¾ä¸åˆ°æŒ‡å®šçš„ä»»å‹™ ID")

    # DBClient å›å‚³çš„å·²ç¶“æ˜¯ dictï¼Œç„¡éœ€è½‰æ›
    response_data = status_info

    # å˜—è©¦è§£æ JSON çµæœ
    if response_data.get("result"):
        try:
            response_data["result"] = json.loads(response_data["result"])
        except json.JSONDecodeError:
            # å¦‚æœä¸æ˜¯åˆæ³•çš„ JSONï¼Œå°±ä»¥åŸå§‹å­—ä¸²å½¢å¼å›å‚³
            log.warning(f"ä»»å‹™ {task_id} çš„çµæœä¸æ˜¯æœ‰æ•ˆçš„ JSON æ ¼å¼ã€‚")
            pass

    log.info(f"âœ… å›å‚³ä»»å‹™ {task_id} çš„ç‹€æ…‹: {response_data['status']}")
    return JSONResponse(content=response_data)


@app.post("/api/log/action", status_code=200)
async def log_action_endpoint(payload: Dict):
    """
    æ¥æ”¶å‰ç«¯ç™¼é€çš„æ“ä½œæ—¥èªŒï¼Œä¸¦é€éè³‡æ–™åº«æ—¥èªŒè™•ç†å™¨è¨˜éŒ„ã€‚
    """
    action = payload.get("action", "unknown_action")
    # ç²å–ä¸€å€‹å°ˆé–€çš„ logger ä¾†æ¨™è­˜é€™äº›æ—¥èªŒçš„ä¾†æºç‚º 'frontend_action'
    # DatabaseLogHandler æœƒæ“·å–é€™å€‹æ—¥èªŒï¼Œä¸¦å°‡å…¶èˆ‡ logger åç¨±ä¸€èµ·å­˜å…¥è³‡æ–™åº«
    action_logger = logging.getLogger('frontend_action')
    action_logger.info(action)

    log.info(f"ğŸ“ å·²å°‡å‰ç«¯æ“ä½œè¨˜éŒ„åˆ°è³‡æ–™åº«: {action}") # åŒæ™‚åœ¨ä¸»æ§å°ä¹Ÿé¡¯ç¤ºæ—¥èªŒ
    return {"status": "logged"}


import psutil

@app.get("/api/application_status")
async def get_application_status():
    """
    ç²å–æ ¸å¿ƒæ‡‰ç”¨çš„ç‹€æ…‹ï¼Œä¾‹å¦‚æ¨¡å‹æ˜¯å¦å·²è¼‰å…¥ã€‚
    """
    # TODO: é€™éƒ¨åˆ†å°‡åœ¨å¾ŒçºŒèˆ‡ worker ç‹€æ…‹åŒæ­¥
    return {
        "model_loaded": False,
        "active_model": None,
        "message": "ç­‰å¾…ä½¿ç”¨è€…æ“ä½œ"
    }

@app.get("/api/system_stats")
async def get_system_stats():
    """
    ç²å–ä¸¦å›å‚³ç•¶å‰çš„ç³»çµ±è³‡æºä½¿ç”¨ç‹€æ…‹ï¼ˆCPU, RAM, GPUï¼‰ã€‚
    """
    # CPU
    cpu_usage = psutil.cpu_percent(interval=0.1)

    # RAM
    ram = psutil.virtual_memory()
    ram_usage = ram.percent

    # GPU (é€é nvidia-smi)
    gpu_usage = None
    gpu_detected = False
    try:
        # åŸ·è¡Œ nvidia-smi å‘½ä»¤
        result = subprocess.run(
            ['nvidia-smi', '--query-gpu=utilization.gpu', '--format=csv,noheader,nounits'],
            capture_output=True, text=True, check=True
        )
        # è§£æè¼¸å‡º
        gpu_usage = float(result.stdout.strip())
        gpu_detected = True
    except (FileNotFoundError, subprocess.CalledProcessError) as e:
        # nvidia-smi ä¸å­˜åœ¨æˆ–åŸ·è¡Œå¤±æ•—
        log.debug(f"ç„¡æ³•ç²å– GPU è³‡è¨Š: {e}")
        gpu_usage = None
        gpu_detected = False

    return {
        "cpu_usage": cpu_usage,
        "ram_usage": ram_usage,
        "gpu_usage": gpu_usage,
        "gpu_detected": gpu_detected,
    }


@app.get("/api/tasks")
async def get_all_tasks_endpoint():
    """
    ç²å–æ‰€æœ‰ä»»å‹™çš„åˆ—è¡¨ï¼Œç”¨æ–¼å‰ç«¯å±•ç¤ºã€‚
    """
    tasks = db_client.get_all_tasks()
    # å˜—è©¦è§£æ payload å’Œ result ä¸­çš„ JSON å­—ä¸²
    for task in tasks:
        try:
            if task.get("payload"):
                task["payload"] = json.loads(task["payload"])
        except (json.JSONDecodeError, TypeError):
            log.warning(f"ä»»å‹™ {task.get('task_id')} çš„ payload ä¸æ˜¯æœ‰æ•ˆçš„ JSONã€‚")
            pass # ä¿æŒåŸæ¨£
        try:
            if task.get("result"):
                task["result"] = json.loads(task["result"])
        except (json.JSONDecodeError, TypeError):
            log.warning(f"ä»»å‹™ {task.get('task_id')} çš„ result ä¸æ˜¯æœ‰æ•ˆçš„ JSONã€‚")
            pass # ä¿æŒåŸæ¨£
    return JSONResponse(content=tasks)


@app.get("/api/logs")
async def get_system_logs_endpoint(
    levels: List[str] = Query(None, alias="level"),
    sources: List[str] = Query(None, alias="source")
):
    """
    ç²å–ç³»çµ±æ—¥èªŒï¼Œå¯æŒ‰ç­‰ç´šå’Œä¾†æºé€²è¡Œç¯©é¸ã€‚
    """
    log.info(f"API: æ­£åœ¨æŸ¥è©¢ç³»çµ±æ—¥èªŒ (Levels: {levels}, Sources: {sources})")
    try:
        logs = db_client.get_system_logs(levels=levels, sources=sources)
        return JSONResponse(content=logs)
    except Exception as e:
        log.error(f"âŒ æŸ¥è©¢ç³»çµ±æ—¥èªŒæ™‚ API å‡ºéŒ¯: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="æŸ¥è©¢ç³»çµ±æ—¥èªŒæ™‚ç™¼ç”Ÿå…§éƒ¨éŒ¯èª¤")


@app.get("/api/download/{task_id}")
async def download_transcript(task_id: str):
    """
    æ ¹æ“šä»»å‹™ ID ä¸‹è¼‰è½‰éŒ„çµæœæª”æ¡ˆã€‚
    """
    task = db_client.get_task_status(task_id)
    if not task:
        raise HTTPException(status_code=404, detail="æ‰¾ä¸åˆ°æŒ‡å®šçš„ä»»å‹™ IDã€‚")

    if task['status'] != 'completed':
        raise HTTPException(status_code=400, detail="ä»»å‹™å°šæœªå®Œæˆï¼Œç„¡æ³•ä¸‹è¼‰ã€‚")

    try:
        # å¾ result æ¬„ä½è§£æå‡ºæª”å
        result_data = json.loads(task['result'])
        # ä¾åºæª¢æŸ¥å¯èƒ½çš„è·¯å¾‘éµåï¼Œä»¥æ”¯æ´æ‰€æœ‰ä»»å‹™é¡å‹
        output_filename = (
            result_data.get("transcript_path") or
            result_data.get("output_path") or
            result_data.get("html_report_path") or
            result_data.get("pdf_report_path")
        )

        if not output_filename:
            raise HTTPException(status_code=500, detail="ä»»å‹™çµæœä¸­æœªåŒ…å«æœ‰æ•ˆçš„æª”æ¡ˆè·¯å¾‘ã€‚")

        file_path = Path(output_filename)
        if not file_path.is_file():
            log.error(f"âŒ è½‰éŒ„æª”æ¡ˆä¸å­˜åœ¨: {file_path}")
            raise HTTPException(status_code=404, detail="è½‰éŒ„æª”æ¡ˆéºå¤±æˆ–ç„¡æ³•è®€å–ã€‚")

        # æä¾›æª”æ¡ˆä¸‹è¼‰
        from fastapi.responses import FileResponse
        ext = file_path.suffix.lower()
        if ext == '.pdf':
            media_type = 'application/pdf'
        elif ext == '.html':
            media_type = 'text/html'
        else:
            media_type = 'text/plain'
        return FileResponse(path=file_path, filename=file_path.name, media_type=media_type)

    except (json.JSONDecodeError, KeyError) as e:
        log.error(f"âŒ è§£æä»»å‹™ {task_id} çš„çµæœæ™‚å‡ºéŒ¯: {e}")
        raise HTTPException(status_code=500, detail="ç„¡æ³•è§£æä»»å‹™çµæœã€‚")


# --- YouTube åŠŸèƒ½ç›¸é—œ API ---

@app.post("/api/youtube/validate_api_key")
async def validate_api_key(request: Request):
    """æ¥æ”¶å‰ç«¯å‚³ä¾†çš„ API Key ä¸¦é€²è¡Œé©—è­‰ã€‚"""
    try:
        payload = await request.json()
        api_key = payload.get("api_key")
        if not api_key:
            raise HTTPException(status_code=400, detail="æœªæä¾› API é‡‘é‘°ã€‚")

        # åœ¨æ¨¡æ“¬æ¨¡å¼ä¸‹ï¼Œåªè¦é‡‘é‘°éç©ºå°±è¦–ç‚ºæœ‰æ•ˆ
        if IS_MOCK_MODE:
            log.info("æ¨¡æ“¬æ¨¡å¼ï¼šå°‡éç©º API é‡‘é‘°è¦–ç‚ºæœ‰æ•ˆã€‚")
            return {"valid": True}

        # çœŸå¯¦æ¨¡å¼ä¸‹ï¼Œå‘¼å«å·¥å…·é€²è¡Œé©—è­‰
        tool_script = "tools/gemini_processor.py"
        cmd = [sys.executable, tool_script, "--command=validate_key"]

        # å°‡é‡‘é‘°ä½œç‚ºç’°å¢ƒè®Šæ•¸å‚³éçµ¦å­ç¨‹åºï¼Œæ›´å®‰å…¨
        env = os.environ.copy()
        env["GOOGLE_API_KEY"] = api_key

        # è¨­å®š check=Falseï¼Œå› ç‚ºæˆ‘å€‘é æœŸåœ¨é‡‘é‘°ç„¡æ•ˆæ™‚ç¨‹åºæœƒå¤±æ•—
        result = subprocess.run(cmd, capture_output=True, text=True, encoding='utf-8', env=env, check=False)

        if result.returncode == 0:
            log.info(f"API é‡‘é‘°é©—è­‰æˆåŠŸã€‚")
            return {"valid": True}
        else:
            log.warning(f"API é‡‘é‘°é©—è­‰å¤±æ•—ã€‚Stderr: {result.stderr.strip()}")
            # å˜—è©¦å¾ stderr ä¸­æå–æ›´å…·é«”çš„éŒ¯èª¤è¨Šæ¯
            error_message = result.stderr.strip()
            if "API key not valid" in error_message:
                detail = "API é‡‘é‘°ç„¡æ•ˆã€‚è«‹æª¢æŸ¥æ‚¨çš„é‡‘é‘°æ˜¯å¦æ­£ç¢ºã€‚"
            else:
                detail = "é‡‘é‘°é©—è­‰å¤±æ•—ï¼Œå¯èƒ½æ˜¯ç¶²è·¯å•é¡Œæˆ–é‡‘é‘°æ¬Šé™ä¸è¶³ã€‚"
            return JSONResponse(status_code=400, content={"valid": False, "detail": detail})

    except Exception as e:
        log.error(f"é©—è­‰ API é‡‘é‘°æ™‚ç™¼ç”Ÿä¼ºæœå™¨å…§éƒ¨éŒ¯èª¤: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"ä¼ºæœå™¨å…§éƒ¨éŒ¯èª¤: {e}")


@app.get("/api/youtube/models")
async def get_youtube_models():
    """ç²å–å¯ç”¨çš„ Gemini æ¨¡å‹åˆ—è¡¨ã€‚"""
    # åœ¨æ¨¡æ“¬æ¨¡å¼ä¸‹ï¼Œå›å‚³ä¸€å€‹å›ºå®šçš„å‡åˆ—è¡¨
    if IS_MOCK_MODE:
        return {
            "models": [
                {"id": "gemini-pro-mock", "name": "Gemini Pro (æ¨¡æ“¬)"},
                {"id": "gemini-1.5-flash-mock", "name": "Gemini 1.5 Flash (æ¨¡æ“¬)"}
            ]
        }

    # çœŸå¯¦æ¨¡å¼ä¸‹ï¼Œå¾ gemini_processor.py ç²å–
    # æ³¨æ„ï¼šæ­¤ç«¯é»ç¾åœ¨ä¾è³´æ–¼ä¸€å€‹æœ‰æ•ˆçš„ GOOGLE_API_KEY ç’°å¢ƒè®Šæ•¸
    try:
        if not os.environ.get("GOOGLE_API_KEY"):
             raise HTTPException(status_code=401, detail="å¾Œç«¯å°šæœªè¨­å®šæœ‰æ•ˆçš„ Google API é‡‘é‘°ã€‚")

        tool_script = "tools/gemini_processor.py"
        cmd = [sys.executable, tool_script, "--command=list_models"]
        result = subprocess.run(cmd, capture_output=True, text=True, check=True, encoding='utf-8')
        models = json.loads(result.stdout)
        return {"models": models}
    except subprocess.CalledProcessError as e:
        log.error(f"ç²å– Gemini æ¨¡å‹åˆ—è¡¨å¤±æ•—ï¼Œå¯èƒ½æ˜¯å› ç‚º API é‡‘é‘°ç„¡æ•ˆã€‚Stderr: {e.stderr}")
        raise HTTPException(status_code=401, detail="ç„¡æ³•ä½¿ç”¨æä¾›çš„ API é‡‘é‘°ç²å–æ¨¡å‹åˆ—è¡¨ã€‚")
    except Exception as e:
        log.error(f"ç²å– Gemini æ¨¡å‹åˆ—è¡¨æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="ç„¡æ³•ç²å– Gemini æ¨¡å‹åˆ—è¡¨ã€‚")


@app.post("/api/youtube/process", status_code=202)
async def process_youtube_urls(request: Request):
    """
    æ¥æ”¶ YouTube URLï¼Œæ ¹æ“š 'download_only' æ——æ¨™æ±ºå®šæ˜¯åƒ…ä¸‹è¼‰éŸ³è¨Šï¼Œé‚„æ˜¯åŸ·è¡Œå®Œæ•´çš„ AI åˆ†ææµç¨‹ã€‚
    """
    payload = await request.json()
    # JULES'S FIX: å‰ç«¯ç™¼é€çš„æ˜¯ 'requests' ç‰©ä»¶é™£åˆ—ï¼Œè€Œé 'urls' å­—ä¸²é™£åˆ—ã€‚ä¿®æ­£é€™è£¡ä»¥æ­£ç¢ºè§£æã€‚
    requests_list = payload.get("requests", [])
    model = payload.get("model")
    download_only = payload.get("download_only", False)

    if not requests_list:
        raise HTTPException(status_code=400, detail="è«‹æ±‚ä¸­å¿…é ˆåŒ…å« 'requests'ã€‚")
    if not download_only and not model:
        raise HTTPException(status_code=400, detail="åŸ·è¡Œå®Œæ•´åˆ†ææ™‚å¿…é ˆæä¾› 'model'ã€‚")

    tasks = []
    for req_item in requests_list:
        url = req_item.get("url")
        filename = req_item.get("filename") # é›–ç„¶ç›®å‰æœªä½¿ç”¨ï¼Œä½†å…ˆè®€å–å‡ºä¾†

        if not url or not url.strip():
            continue

        task_id = str(uuid.uuid4())

        if download_only:
            # åƒ…ä¸‹è¼‰æ¨¡å¼ï¼šåªå»ºç«‹ä¸€å€‹ä¸‹è¼‰ä»»å‹™
            # å°‡è‡ªè¨‚æª”åå­˜å…¥ payloadï¼Œä»¥ä¾¿æœªä¾†ä½¿ç”¨
            task_payload = {"url": url, "output_dir": str(UPLOADS_DIR), "custom_filename": filename}
            db_client.add_task(task_id, json.dumps(task_payload), task_type='youtube_download_only')
            tasks.append({"url": url, "task_id": task_id})
        else:
            # å®Œæ•´åˆ†ææ¨¡å¼ï¼šå»ºç«‹ä¸‹è¼‰å’Œè™•ç†å…©å€‹ä»»å‹™
            download_task_id = task_id
            process_task_id = str(uuid.uuid4())

            download_payload = {"url": url, "output_dir": str(UPLOADS_DIR), "custom_filename": filename}
            process_payload = {"model": model, "output_dir": "transcripts"}

            db_client.add_task(download_task_id, json.dumps(download_payload), task_type='youtube_download')
            db_client.add_task(process_task_id, json.dumps(process_payload), task_type='gemini_process', depends_on=download_task_id)

            tasks.append({"url": url, "task_id": download_task_id})

    return JSONResponse(content={"message": f"å·²ç‚º {len(tasks)} å€‹ URL å»ºç«‹è™•ç†ä»»å‹™ã€‚", "tasks": tasks})


def trigger_model_download(model_size: str, loop: asyncio.AbstractEventLoop):
    """
    åœ¨ä¸€å€‹å–®ç¨çš„åŸ·è¡Œç·’ä¸­åŸ·è¡Œæ¨¡å‹ä¸‹è¼‰ï¼Œä¸¦é€é WebSocket å›å ±çµæœã€‚
    é€™å€‹ç‰ˆæœ¬æœƒé€è¡Œè®€å– stdout ä¾†ç²å–å³æ™‚çš„ JSON é€²åº¦æ›´æ–°ã€‚
    """
    def _download_in_thread():
        log.info(f"ğŸ§µ [åŸ·è¡Œç·’] é–‹å§‹ä¸‹è¼‰æ¨¡å‹: {model_size}")
        try:
            tool_script = "tools/mock_transcriber.py" if IS_MOCK_MODE else "tools/transcriber.py"
            cmd = [sys.executable, tool_script, "--command=download", f"--model_size={model_size}"]

            process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
                encoding='utf-8',
                bufsize=1 # Line-buffered
            )

            # é€è¡Œè®€å– stdout ä»¥ç²å–é€²åº¦æ›´æ–°
            if process.stdout:
                for line in iter(process.stdout.readline, ''):
                    line = line.strip()
                    if not line:
                        continue
                    try:
                        data = json.loads(line)
                        # å»ºç«‹ WebSocket è¨Šæ¯
                        message = {
                            "type": "DOWNLOAD_STATUS",
                            "payload": {
                                "model": model_size,
                                "status": "downloading",
                                **data  # é€™æœƒåŒ…å« 'type', 'percent', 'description' ç­‰
                            }
                        }
                        asyncio.run_coroutine_threadsafe(manager.broadcast_json(message), loop)
                    except json.JSONDecodeError:
                        log.warning(f"[åŸ·è¡Œç·’] ç„¡æ³•è§£æä¾†è‡ª transcriber çš„ä¸‹è¼‰é€²åº¦ JSON: {line}")

            process.wait() # ç­‰å¾…ç¨‹åºçµæŸ

            # æ ¹æ“šç¨‹åºçš„è¿”å›ç¢¼æ±ºå®šæœ€çµ‚ç‹€æ…‹
            if process.returncode == 0:
                log.info(f"âœ… [åŸ·è¡Œç·’] æ¨¡å‹ '{model_size}' ä¸‹è¼‰æˆåŠŸã€‚")
                message = {
                    "type": "DOWNLOAD_STATUS",
                    "payload": {"model": model_size, "status": "completed", "progress": 100}
                }
            else:
                stderr_output = process.stderr.read() if process.stderr else "N/A"
                log.error(f"âŒ [åŸ·è¡Œç·’] æ¨¡å‹ '{model_size}' ä¸‹è¼‰å¤±æ•—ã€‚ Stderr: {stderr_output}")
                message = {
                    "type": "DOWNLOAD_STATUS",
                    "payload": {"model": model_size, "status": "failed", "error": stderr_output}
                }

            asyncio.run_coroutine_threadsafe(manager.broadcast_json(message), loop)

        except Exception as e:
            log.error(f"âŒ [åŸ·è¡Œç·’] ä¸‹è¼‰åŸ·è¡Œç·’ä¸­ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}", exc_info=True)
            message = {
                "type": "DOWNLOAD_STATUS",
                "payload": {"model": model_size, "status": "failed", "error": str(e)}
            }
            asyncio.run_coroutine_threadsafe(manager.broadcast_json(message), loop)

    # å»ºç«‹ä¸¦å•Ÿå‹•åŸ·è¡Œç·’
    thread = threading.Thread(target=_download_in_thread)
    thread.start()


def trigger_transcription(task_id: str, file_path: str, model_size: str, language: Optional[str], beam_size: int, loop: asyncio.AbstractEventLoop, original_filename: Optional[str] = None):
    """
    åœ¨ä¸€å€‹å–®ç¨çš„åŸ·è¡Œç·’ä¸­åŸ·è¡Œè½‰éŒ„ï¼Œä¸¦é€é WebSocket å³æ™‚ä¸²æµçµæœã€‚
    """
    def _transcribe_in_thread():
        display_name = original_filename or file_path
        log.info(f"ğŸ§µ [åŸ·è¡Œç·’] é–‹å§‹è™•ç†è½‰éŒ„ä»»å‹™: {task_id}ï¼Œæª”æ¡ˆ: {display_name}")

        # æº–å‚™ä¸€å€‹å‡çš„è¼¸å‡ºæª”æ¡ˆè·¯å¾‘ï¼Œå› ç‚º transcriber.py éœ€è¦å®ƒï¼Œä½†æˆ‘å€‘å¯¦éš›ä¸Šæ˜¯å¾ stdout è®€å–
        output_dir = ROOT_DIR / "transcripts"
        output_dir.mkdir(exist_ok=True)
        dummy_output_path = output_dir / f"{task_id}.txt"

        try:
            # JULES'S FIX: å¢åŠ ä¸€å€‹ç’°å¢ƒè®Šæ•¸ä¾†å¼·åˆ¶ä½¿ç”¨æ¨¡æ“¬è½‰éŒ„å™¨ï¼Œä»¥æ”¯æ´æ··åˆæ¨¡å¼æ¸¬è©¦
            force_mock = os.environ.get("FORCE_MOCK_TRANSCRIBER") == "true"
            tool_script = "tools/mock_transcriber.py" if IS_MOCK_MODE or force_mock else "tools/transcriber.py"
            cmd = [
                sys.executable,
                tool_script,
                "--command=transcribe",
                f"--audio_file={file_path}",
                f"--output_file={dummy_output_path}",
                f"--model_size={model_size}",
            ]
            if language:
                cmd.append(f"--language={language}")
            # æ–°å¢ beam_size åƒæ•¸
            cmd.append(f"--beam_size={beam_size}")

            log.info(f"åŸ·è¡Œè½‰éŒ„æŒ‡ä»¤: {' '.join(map(str, cmd))}")

            process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
                encoding='utf-8',
                bufsize=1 # Line-buffered
            )

            # JULES'S FIX: Use the original filename for the UI if available.
            start_message_filename = original_filename or Path(file_path).name
            start_message = {
                "type": "TRANSCRIPTION_STATUS",
                "payload": {"task_id": task_id, "status": "starting", "filename": start_message_filename}
            }
            asyncio.run_coroutine_threadsafe(manager.broadcast_json(start_message), loop)

            if process.stdout:
                for line in iter(process.stdout.readline, ''):
                    line = line.strip()
                    if not line:
                        continue

                    try:
                        data = json.loads(line)
                        # JULES' FIX: åªè½‰ç™¼ 'segment' é¡å‹çš„è¨Šæ¯ä»¥é¿å…éé æœŸçš„ UI å…ƒç´ 
                        if data.get("type") == "segment":
                            message = {
                                "type": "TRANSCRIPTION_UPDATE",
                                "payload": {"task_id": task_id, **data}
                            }
                            asyncio.run_coroutine_threadsafe(manager.broadcast_json(message), loop)
                    except json.JSONDecodeError:
                        log.warning(f"[åŸ·è¡Œç·’] ç„¡æ³•è§£æä¾†è‡ª transcriber çš„ JSON è¡Œ: {line}")

            process.wait()

            if process.returncode == 0:
                log.info(f"âœ… [åŸ·è¡Œç·’] è½‰éŒ„ä»»å‹™ '{task_id}' æˆåŠŸå®Œæˆã€‚")

                # è®€å–çµæœä¸¦æ›´æ–°è³‡æ–™åº«ç‹€æ…‹
                final_transcript = dummy_output_path.read_text(encoding='utf-8').strip()
                final_result_obj = {
                    "transcript": final_transcript,
                    "transcript_path": str(dummy_output_path)
                }
                db_client.update_task_status(task_id, 'completed', json.dumps(final_result_obj))
                log.info(f"âœ… [åŸ·è¡Œç·’] å·²å°‡ä»»å‹™ {task_id} çš„ç‹€æ…‹å’Œçµæœæ›´æ–°è‡³è³‡æ–™åº«ã€‚")

                final_message = {
                    "type": "TRANSCRIPTION_STATUS",
                    "payload": {"task_id": task_id, "status": "completed", "result": final_result_obj}
                }
            else:
                stderr_output = process.stderr.read() if process.stderr else "N/A"
                log.error(f"âŒ [åŸ·è¡Œç·’] è½‰éŒ„ä»»å‹™ '{task_id}' å¤±æ•—ã€‚è¿”å›ç¢¼: {process.returncode}ã€‚Stderr: {stderr_output}")
                final_message = {
                    "type": "TRANSCRIPTION_STATUS",
                    "payload": {"task_id": task_id, "status": "failed", "error": stderr_output}
                }

            asyncio.run_coroutine_threadsafe(manager.broadcast_json(final_message), loop)

        except Exception as e:
            log.error(f"âŒ [åŸ·è¡Œç·’] è½‰éŒ„åŸ·è¡Œç·’ä¸­ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}", exc_info=True)
            error_message = {
                "type": "TRANSCRIPTION_STATUS",
                "payload": {"task_id": task_id, "status": "failed", "error": str(e)}
            }
            asyncio.run_coroutine_threadsafe(manager.broadcast_json(error_message), loop)

    thread = threading.Thread(target=_transcribe_in_thread)
    thread.start()


def trigger_youtube_processing(task_id: str, loop: asyncio.AbstractEventLoop):
    """åœ¨ä¸€å€‹å–®ç¨çš„åŸ·è¡Œç·’ä¸­åŸ·è¡Œ YouTube è™•ç†æµç¨‹ã€‚"""
    def _process_in_thread():
        log.info(f"ğŸ§µ [åŸ·è¡Œç·’] é–‹å§‹è™•ç† YouTube ä»»å‹™ï¼ŒID: {task_id}")

        task_info = db_client.get_task_status(task_id)
        if not task_info:
            log.error(f"âŒ [åŸ·è¡Œç·’] æ‰¾ä¸åˆ°ä»»å‹™ {task_id}")
            return

        task_type = task_info.get('type')

        try:
            # --- æ­¥é©Ÿ 1: ä¸‹è¼‰éŸ³è¨Š (æ‰€æœ‰ YouTube ä»»å‹™éƒ½éœ€è¦) ---
            payload = json.loads(task_info['payload'])
            url = payload['url']

            asyncio.run_coroutine_threadsafe(manager.broadcast_json({
                "type": "YOUTUBE_STATUS",
                "payload": {"task_id": task_id, "status": "downloading", "message": f"æ­£åœ¨ä¸‹è¼‰: {url}", "task_type": task_type}
            }), loop)

            tool_script = "tools/mock_youtube_downloader.py" if IS_MOCK_MODE else "tools/youtube_downloader.py"
            cmd = [sys.executable, tool_script, "--url", url, "--output-dir", str(UPLOADS_DIR)]

            process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, encoding='utf-8')

            last_line = ""
            if process.stdout:
                for line in iter(process.stdout.readline, ''):
                    line = line.strip()
                    if line: last_line = line

            process.wait()
            if process.returncode != 0:
                stderr_output = process.stderr.read() if process.stderr else "N/A"
                raise RuntimeError(f"YouTube downloader failed: {stderr_output}")

            download_result = json.loads(last_line)
            audio_file_path = download_result['output_path']
            video_title = download_result.get('video_title', 'ç„¡æ¨™é¡Œå½±ç‰‡')
            log.info(f"âœ… [åŸ·è¡Œç·’] YouTube éŸ³è¨Šä¸‹è¼‰å®Œæˆ: {audio_file_path}")

            # --- æ­¥é©Ÿ 2: æ ¹æ“šä»»å‹™é¡å‹æ±ºå®šä¸‹ä¸€æ­¥ ---
            if task_type == 'youtube_download_only':
                # å¦‚æœåªæ˜¯ä¸‹è¼‰ä»»å‹™ï¼Œåˆ°æ­¤çµæŸ
                db_client.update_task_status(task_id, 'completed', json.dumps(download_result))
                log.info(f"âœ… [åŸ·è¡Œç·’] 'åƒ…ä¸‹è¼‰éŸ³è¨Š' ä»»å‹™ {task_id} å®Œæˆã€‚")

                asyncio.run_coroutine_threadsafe(manager.broadcast_json({
                    "type": "YOUTUBE_STATUS",
                    "payload": {"task_id": task_id, "status": "completed", "result": download_result, "task_type": "download_only"}
                }), loop)
                return # çµæŸåŸ·è¡Œç·’

            # --- æ­¥é©Ÿ 3: åŸ·è¡Œ Gemini AI åˆ†æ (åƒ…é©ç”¨æ–¼å®Œæ•´æµç¨‹) ---
            db_client.update_task_status(task_id, 'completed', json.dumps(download_result))

            dependent_task_id = db_client.find_dependent_task(task_id)
            if not dependent_task_id:
                raise ValueError(f"æ‰¾ä¸åˆ°ä¾è³´æ–¼ {task_id} çš„ gemini_process ä»»å‹™")

            process_task_info = db_client.get_task_status(dependent_task_id)
            process_payload = json.loads(process_task_info['payload'])
            model = process_payload['model']

            asyncio.run_coroutine_threadsafe(manager.broadcast_json({
                "type": "YOUTUBE_STATUS",
                "payload": {"task_id": dependent_task_id, "status": "processing", "message": f"ä½¿ç”¨ {model} é€²è¡Œ AI åˆ†æ...", "task_type": "gemini_process"}
            }), loop)

            tool_script = "tools/mock_gemini_processor.py" if IS_MOCK_MODE else "tools/gemini_processor.py"
            report_output_dir = ROOT_DIR / "transcripts"
            report_output_dir.mkdir(exist_ok=True)

            cmd = [
                sys.executable, tool_script,
                "--audio-file", audio_file_path,
                "--model", model,
                "--output-dir", str(report_output_dir),
                "--video-title", video_title
            ]

            result = subprocess.run(cmd, capture_output=True, text=True, check=True, encoding='utf-8')
            process_result = json.loads(result.stdout)

            db_client.update_task_status(dependent_task_id, 'completed', json.dumps(process_result))
            log.info(f"âœ… [åŸ·è¡Œç·’] Gemini AI è™•ç†å®Œæˆã€‚")

            asyncio.run_coroutine_threadsafe(manager.broadcast_json({
                "type": "YOUTUBE_STATUS",
                "payload": {"task_id": dependent_task_id, "status": "completed", "result": process_result, "task_type": "gemini_process"}
            }), loop)

        except Exception as e:
            log.error(f"âŒ [åŸ·è¡Œç·’] YouTube è™•ç†éˆä¸­ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
            # ç¢ºå®šè¦æ›´æ–°å“ªå€‹ä»»å‹™ç‚ºå¤±æ•—ç‹€æ…‹
            failed_task_id = 'dependent_task_id' if 'dependent_task_id' in locals() and dependent_task_id else task_id
            db_client.update_task_status(failed_task_id, 'failed', json.dumps({"error": str(e)}))
            asyncio.run_coroutine_threadsafe(manager.broadcast_json({
                "type": "YOUTUBE_STATUS",
                "payload": {"task_id": failed_task_id, "status": "failed", "error": str(e)}
            }), loop)

    thread = threading.Thread(target=_process_in_thread)
    thread.start()


@app.get("/api/debug/latest_frontend_action_log")
async def get_latest_frontend_action_log():
    """
    [åƒ…ä¾›æ¸¬è©¦] ç²å–æœ€æ–°çš„å‰ç«¯æ“ä½œæ—¥èªŒã€‚
    ç”¨æ–¼ E2E æ¸¬è©¦ï¼Œä»¥é©—è­‰æ—¥èªŒæ˜¯å¦å·²æˆåŠŸå¯«å…¥è³‡æ–™åº«ã€‚
    """
    try:
        # æˆ‘å€‘åªé—œå¿ƒä¾†è‡ª 'frontend_action' logger çš„æ—¥èªŒ
        logs = db_client.get_system_logs(sources=['frontend_action'])
        if not logs:
            # å¦‚æœæ²’æœ‰æ—¥èªŒï¼Œè¿”å›ä¸€å€‹æ¸…æ™°çš„ç©ºå›æ‡‰ï¼Œè€Œä¸æ˜¯ 404
            return JSONResponse(content={"latest_log": None}, status_code=200)

        # get_system_logs æŒ‰æ™‚é–“æˆ³å‡åºæ’åºï¼Œæ‰€ä»¥æœ€å¾Œä¸€å€‹å°±æ˜¯æœ€æ–°çš„
        latest_log = logs[-1]
        return JSONResponse(content={"latest_log": latest_log})
    except Exception as e:
        log.error(f"âŒ æŸ¥è©¢æœ€æ–°å‰ç«¯æ—¥èªŒæ™‚å‡ºéŒ¯: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="æŸ¥è©¢æœ€æ–°å‰ç«¯æ—¥èªŒæ™‚ç™¼ç”Ÿå…§éƒ¨éŒ¯èª¤")


@app.websocket("/api/ws")
async def websocket_endpoint(websocket: WebSocket):
    await manager.connect(websocket)
    try:
        while True:
            data = await websocket.receive_text()
            log.info(f"å¾ WebSocket æ”¶åˆ°è¨Šæ¯: {data}")

            try:
                message = json.loads(data)
                msg_type = message.get("type")
                payload = message.get("payload", {})

                if msg_type == "DOWNLOAD_MODEL":
                    model_size = payload.get("model")
                    if model_size:
                        log.info(f"æ”¶åˆ°ä¸‹è¼‰ '{model_size}' æ¨¡å‹çš„è«‹æ±‚ã€‚")
                        await manager.broadcast_json({
                            "type": "DOWNLOAD_STATUS",
                            "payload": {"model": model_size, "status": "starting", "progress": 0}
                        })
                        loop = asyncio.get_running_loop()
                        trigger_model_download(model_size, loop)
                    else:
                        await manager.broadcast_json({"type": "ERROR", "payload": "ç¼ºå°‘æ¨¡å‹å¤§å°åƒæ•¸"})

                elif msg_type == "START_TRANSCRIPTION":
                    task_id = payload.get("task_id")
                    if not task_id:
                        await manager.broadcast_json({"type": "ERROR", "payload": "ç¼ºå°‘ task_id åƒæ•¸"})
                        continue

                    task_info = db_client.get_task_status(task_id)
                    if not task_info:
                        await manager.broadcast_json({"type": "ERROR", "payload": f"æ‰¾ä¸åˆ°ä»»å‹™ {task_id}"})
                        continue

                    try:
                        task_payload = json.loads(task_info['payload'])
                        file_path = task_payload.get("input_file")
                        model_size = task_payload.get("model_size", "tiny")
                        language = task_payload.get("language")
                        beam_size = task_payload.get("beam_size", 5)
                        original_filename = task_payload.get("original_filename") # JULES'S FIX
                    except (json.JSONDecodeError, KeyError) as e:
                        await manager.broadcast_json({"type": "ERROR", "payload": f"è§£æä»»å‹™ {task_id} çš„ payload å¤±æ•—: {e}"})
                        continue

                    if not file_path:
                        await manager.broadcast_json({"type": "ERROR", "payload": "ä»»å‹™ payload ä¸­ç¼ºå°‘æª”æ¡ˆè·¯å¾‘"})
                    else:
                        display_name = original_filename or file_path
                        log.info(f"æ”¶åˆ°é–‹å§‹è½‰éŒ„ '{display_name}' çš„è«‹æ±‚ (ä¾†è‡ªä»»å‹™ {task_id})ã€‚")
                        loop = asyncio.get_running_loop()
                        trigger_transcription(task_id, file_path, model_size, language, beam_size, loop, original_filename=original_filename)

                elif msg_type == "START_YOUTUBE_PROCESSING":
                    task_id = payload.get("task_id") # This is the download_task_id
                    if not task_id:
                        await manager.broadcast_json({"type": "ERROR", "payload": "ç¼ºå°‘ task_id åƒæ•¸"})
                        continue

                    log.info(f"æ”¶åˆ°é–‹å§‹è™•ç† YouTube ä»»å‹™éˆçš„è«‹æ±‚ (èµ·å§‹ä»»å‹™ ID: {task_id})ã€‚")
                    loop = asyncio.get_running_loop()
                    trigger_youtube_processing(task_id, loop)

                else:
                    await manager.broadcast_json({
                        "type": "ECHO",
                        "payload": f"å·²æ”¶åˆ°æœªçŸ¥é¡å‹çš„è¨Šæ¯: {msg_type}"
                    })

            except json.JSONDecodeError:
                log.error("æ”¶åˆ°äº†é JSON æ ¼å¼çš„ WebSocket è¨Šæ¯ã€‚")
                await manager.broadcast_json({"type": "ERROR", "payload": "è¨Šæ¯å¿…é ˆæ˜¯ JSON æ ¼å¼"})

    except WebSocketDisconnect:
        manager.disconnect(websocket)
        log.info("WebSocket ç”¨æˆ¶ç«¯å·²é›¢ç·šã€‚")
    except Exception as e:
        log.error(f"WebSocket ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤: {e}", exc_info=True)
        # ç¢ºä¿åœ¨ç™¼ç”ŸéŒ¯èª¤æ™‚ä¹Ÿä¸­æ–·é€£ç·š
        if websocket in manager.active_connections:
            manager.disconnect(websocket)


@app.get("/api/health")
async def health_check():
    """æä¾›ä¸€å€‹ç°¡å–®çš„å¥åº·æª¢æŸ¥ç«¯é»ã€‚"""
    return {"status": "ok", "message": "API Server is running."}


@app.post("/api/internal/notify_task_update", status_code=200)
async def notify_task_update(payload: Dict):
    """
    ä¸€å€‹å…§éƒ¨ç«¯é»ï¼Œä¾› Worker ç¨‹åºåœ¨ä»»å‹™å®Œæˆæ™‚å‘¼å«ï¼Œ
    ä»¥ä¾¿é€é WebSocket å°‡æ›´æ–°å»£æ’­çµ¦å‰ç«¯ã€‚
    """
    task_id = payload.get("task_id")
    status = payload.get("status")
    result = payload.get("result")
    log.info(f"ğŸ”” æ”¶åˆ°ä¾†è‡ª Worker çš„ä»»å‹™æ›´æ–°é€šçŸ¥: Task {task_id} -> {status}")

    # ç¢ºä¿ result æ˜¯å­—å…¸æ ¼å¼
    if isinstance(result, str):
        try:
            result = json.loads(result)
        except json.JSONDecodeError:
            log.warning(f"ä¾†è‡ª worker çš„ä»»å‹™ {task_id} çµæœä¸æ˜¯æœ‰æ•ˆçš„ JSON æ ¼å¼ã€‚")

    message = {
        "type": "TRANSCRIPTION_STATUS",
        "payload": {
            "task_id": task_id,
            "status": status,
            "result": result
        }
    }
    await manager.broadcast_json(message)
    return {"status": "notification_sent"}


# --- ä¸»ç¨‹å¼å•Ÿå‹• ---
if __name__ == "__main__":
    import uvicorn
    import argparse

    parser = argparse.ArgumentParser(description="é³³å‡°éŸ³è¨Šè½‰éŒ„å„€ API ä¼ºæœå™¨")
    parser.add_argument(
        "--port",
        type=int,
        default=8001,
        help="ä¼ºæœå™¨ç›£è½çš„åŸ è™Ÿ"
    )
    args, _ = parser.parse_known_args()

    # JULES: ç§»é™¤æ­¤è™•çš„è³‡æ–™åº«åˆå§‹åŒ–å‘¼å«ã€‚
    # çˆ¶ç¨‹åº orchestrator.py å°‡æœƒè² è²¬æ­¤äº‹ï¼Œä»¥é¿å…ç«¶çˆ­æ¢ä»¶ã€‚

    # JULES'S FIX: The database logging is now set up via the app's lifespan event.
    # setup_database_logging() is no longer needed here.

    log.info("ğŸš€ å•Ÿå‹• API ä¼ºæœå™¨ (v3)...")
    log.info(f"è«‹åœ¨ç€è¦½å™¨ä¸­é–‹å•Ÿ http://127.0.0.1:{args.port}")
    uvicorn.run(app, host="0.0.0.0", port=args.port)
