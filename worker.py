# worker.py
import time
import logging
import json
import subprocess
import sys
import argparse
from pathlib import Path

# å°‡å°ˆæ¡ˆæ ¹ç›®éŒ„åŠ å…¥ sys.path
ROOT_DIR = Path(__file__).resolve().parent
sys.path.insert(0, str(ROOT_DIR))

from db import database

# --- æ—¥èªŒè¨­å®š ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
log = logging.getLogger('worker')

# --- è·¯å¾‘è¨­å®š ---
TOOLS_DIR = ROOT_DIR / "tools"
TRANSCRIPTS_DIR = ROOT_DIR / "transcripts"

def process_task(task: dict, use_mock: bool):
    """
    è™•ç†å–®ä¸€è½‰éŒ„ä»»å‹™ã€‚

    :param task: å¾è³‡æ–™åº«ç²å–çš„ä»»å‹™å­—å…¸ã€‚
    :param use_mock: æ˜¯å¦ä½¿ç”¨æ¨¡æ“¬è½‰éŒ„å·¥å…·ã€‚
    """
    task_id = task['task_id']
    log.info(f"ğŸš€ é–‹å§‹è™•ç†ä»»å‹™: {task_id}")

    try:
        payload = json.loads(task['payload'])
        input_file = Path(payload['input_file'])
        model_size = payload.get('model_size', 'tiny')
        language = payload.get('language') # å¯ä»¥æ˜¯ None

        if not input_file.exists():
            raise FileNotFoundError(f"è¼¸å…¥æª”æ¡ˆä¸å­˜åœ¨: {input_file}")

        # 1. æ±ºå®šè¦ä½¿ç”¨çš„å·¥å…·
        tool_script_name = "mock_transcriber.py" if use_mock else "transcriber.py"
        tool_script_path = TOOLS_DIR / tool_script_name
        if not tool_script_path.exists():
             raise FileNotFoundError(f"å·¥å…·è…³æœ¬ä¸å­˜åœ¨: {tool_script_path}")

        # 2. æº–å‚™è¼¸å‡ºè·¯å¾‘
        TRANSCRIPTS_DIR.mkdir(exist_ok=True)
        output_file = TRANSCRIPTS_DIR / f"{task_id}.txt"

        # 3. æ§‹å»ºä¸¦åŸ·è¡Œå‘½ä»¤
        command = [
            sys.executable, # ä½¿ç”¨èˆ‡ worker ç›¸åŒçš„ Python è§£è­¯å™¨
            str(tool_script_path),
            str(input_file),
            str(output_file),
            f"--model_size={model_size}"
        ]
        if language:
            command.append(f"--language={language}")

        log.info(f"ğŸ”§ åŸ·è¡Œå‘½ä»¤: {' '.join(command)}")

        # æ”¹ç”¨ Popen é€²è¡Œéé˜»å¡å¼è®€å–
        process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, encoding='utf-8')

        # 4. å³æ™‚è®€å– stdout ä¾†æ›´æ–°é€²åº¦
        full_stdout = []
        full_stderr = []

        # ä½¿ç”¨ç·’ä¾†é¿å…é˜»å¡
        def read_stderr():
            for line in process.stderr:
                full_stderr.append(line)
                log.warning(f"[å·¥å…· stderr] {line.strip()}")

        import threading
        stderr_thread = threading.Thread(target=read_stderr)
        stderr_thread.start()

        for line in process.stdout:
            full_stdout.append(line)
            try:
                # è§£æ JSON é€²åº¦
                progress_data = json.loads(line)
                progress = progress_data.get("progress")
                text = progress_data.get("text")
                if progress is not None:
                    log.info(f"ğŸ“ˆ ä»»å‹™ {task_id} é€²åº¦: {progress}% - {text[:30]}...")
                    database.update_task_progress(task_id, progress, text)
            except json.JSONDecodeError:
                # ä¸æ˜¯ JSON æ ¼å¼çš„æ—¥èªŒï¼Œç›´æ¥å°å‡º
                log.info(f"[å·¥å…· stdout] {line.strip()}")

        process.wait()
        stderr_thread.join()

        # 5. è™•ç†æœ€çµ‚çµæœ
        if process.returncode == 0:
            log.info(f"âœ… å·¥å…·æˆåŠŸå®Œæˆä»»å‹™: {task_id}")
            final_transcript = output_file.read_text(encoding='utf-8').strip()
            final_result = json.dumps({
                "transcript": final_transcript,
                "tool_stdout": "".join(full_stdout),
            })
            database.update_task_status(task_id, 'completed', final_result)
        else:
            log.error(f"âŒ å·¥å…·åŸ·è¡Œä»»å‹™å¤±æ•—: {task_id}ã€‚è¿”å›ç¢¼: {process.returncode}")
            error_message = "".join(full_stderr) or "".join(full_stdout) or "æœªçŸ¥éŒ¯èª¤"
            final_result = json.dumps({
                "error": error_message,
                "tool_stdout": "".join(full_stdout),
                "tool_stderr": "".join(full_stderr)
            })
            database.update_task_status(task_id, 'failed', final_result)

    except Exception as e:
        log.critical(f"ğŸ’¥ è™•ç†ä»»å‹™ {task_id} æ™‚ç™¼ç”Ÿæœªé æœŸçš„åš´é‡éŒ¯èª¤: {e}", exc_info=True)
        database.update_task_status(task_id, 'failed', json.dumps({"error": str(e)}))


def main_loop(use_mock: bool, poll_interval: int):
    """
    å·¥äººçš„ä¸»è¿´åœˆï¼ŒæŒçºŒå¾ä½‡åˆ—ä¸­æ‹‰å–ä¸¦è™•ç†ä»»å‹™ã€‚
    """
    log.info(f"ğŸ¤– Worker å·²å•Ÿå‹•ã€‚æ¨¡å¼: {'æ¨¡æ“¬ (Mock)' if use_mock else 'çœŸå¯¦ (Real)'}ã€‚æŸ¥è©¢é–“éš”: {poll_interval} ç§’ã€‚")
    try:
        while True:
            task = database.fetch_and_lock_task()
            if task:
                process_task(task, use_mock)
            else:
                # ä½‡åˆ—ç‚ºç©ºï¼Œç¨ä½œç­‰å¾…
                time.sleep(poll_interval)
    except KeyboardInterrupt:
        log.info("ğŸ›‘ æ”¶åˆ°ä¸­æ–·ä¿¡è™Ÿï¼ŒWorker æ­£åœ¨é—œé–‰...")
    except Exception as e:
        log.critical(f"ğŸ”¥ Worker ä¸»è¿´åœˆç™¼ç”Ÿè‡´å‘½éŒ¯èª¤: {e}", exc_info=True)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="èƒŒæ™¯å·¥ä½œè™•ç†å™¨ã€‚")
    parser.add_argument(
        "--mock",
        action="store_true",
        help="å¦‚æœè¨­ç½®æ­¤æ——æ¨™ï¼Œå‰‡ä½¿ç”¨ mock_transcriber.py é€²è¡Œæ¸¬è©¦ã€‚"
    )
    parser.add_argument(
        "--poll-interval",
        type=int,
        default=5,
        help="ç•¶ä½‡åˆ—ç‚ºç©ºæ™‚ï¼Œè¼ªè©¢è³‡æ–™åº«çš„é–“éš”æ™‚é–“ï¼ˆç§’ï¼‰ã€‚"
    )
    args = parser.parse_args()

    # åœ¨å•Ÿå‹•ä¸»è¿´åœˆä¹‹å‰ï¼Œå…ˆç¢ºä¿è³‡æ–™åº«å·²åˆå§‹åŒ–
    database.initialize_database()

    main_loop(args.mock, args.poll_interval)
