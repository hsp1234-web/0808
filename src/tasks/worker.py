# worker.py
import time
import logging
import json
import subprocess
import sys
import argparse
import requests
import os
import shutil
from pathlib import Path

# å°‡å°ˆæ¡ˆæ ¹ç›®éŒ„åŠ å…¥ sys.path
# å› ç‚ºæ­¤æª”æ¡ˆç¾åœ¨ä½æ–¼ src/tasks/ ä¸­ï¼Œæ‰€ä»¥æ ¹ç›®éŒ„æ˜¯å…¶ä¸Šä¸Šå±¤ç›®éŒ„
ROOT_DIR = Path(__file__).resolve().parent.parent.parent
# sys.path hack ä¸å†éœ€è¦ï¼Œå› ç‚ºæˆ‘å€‘ç¾åœ¨ä½¿ç”¨ `pip install -e .`
# sys.path.insert(0, str(ROOT_DIR))

# JULES'S REFACTOR: Use the database client instead of direct access
# from db import database
from db.client import get_client

# --- æ—¥èªŒè¨­å®š ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
log = logging.getLogger('worker')

def setup_database_logging():
    """è¨­å®šè³‡æ–™åº«æ—¥èªŒè™•ç†å™¨ã€‚"""
    try:
        from db.log_handler import DatabaseLogHandler
        root_logger = logging.getLogger()
        if not any(isinstance(h, DatabaseLogHandler) for h in root_logger.handlers):
            root_logger.addHandler(DatabaseLogHandler(source='worker'))
            log.info("è³‡æ–™åº«æ—¥èªŒè™•ç†å™¨è¨­å®šå®Œæˆ (source: worker)ã€‚")
    except Exception as e:
        log.error(f"æ•´åˆè³‡æ–™åº«æ—¥èªŒæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)

# --- è·¯å¾‘è¨­å®š ---
TOOLS_DIR = ROOT_DIR / "src" / "tools"
UPLOADS_DIR = ROOT_DIR / "uploads"
TRANSCRIPTS_DIR = ROOT_DIR / "transcripts" # èˆŠè·¯å¾‘ï¼Œå¯è€ƒæ…®æœªä¾†é‡æ§‹
# ç¢ºä¿ä¸Šå‚³ç›®éŒ„å­˜åœ¨
UPLOADS_DIR.mkdir(exist_ok=True)


# --- DB å®¢æˆ¶ç«¯ ---
db_client = get_client()

# --- è¼”åŠ©å‡½å¼ ---

def convert_to_media_url(absolute_path_str: str) -> str:
    """å°‡çµ•å°æª”æ¡ˆç³»çµ±è·¯å¾‘è½‰æ›ç‚ºå¯å…¬é–‹å­˜å–çš„ /media URLã€‚"""
    try:
        absolute_path = Path(absolute_path_str)
        # Find the path relative to the UPLOADS_DIR
        relative_path = absolute_path.relative_to(UPLOADS_DIR)
        # Join with /media/ and convert backslashes to forward slashes for URL
        return f"/media/{relative_path.as_posix()}"
    except (ValueError, TypeError):
        log.warning(f"ç„¡æ³•å°‡è·¯å¾‘ {absolute_path_str} è½‰æ›ç‚ºåª’é«” URLã€‚å›å‚³åŸå§‹è·¯å¾‘ã€‚")
        return absolute_path_str

def convert_media_url_to_path(media_url: str) -> Path:
    """å°‡ /media URL è½‰æ›å›ä¼ºæœå™¨ä¸Šçš„çµ•å°æª”æ¡ˆç³»çµ±è·¯å¾‘ã€‚"""
    if not media_url.startswith('/media/'):
        raise ValueError("ç„¡æ•ˆçš„åª’é«” URLï¼Œå¿…é ˆä»¥ /media/ é–‹é ­ã€‚")
    # ç§»é™¤ '/media/' å‰ç¶´ä¸¦èˆ‡ä¸Šå‚³ç›®éŒ„åˆä½µ
    relative_path = media_url.lstrip('/media/')
    return UPLOADS_DIR / relative_path

def notify_api_server(task_id: str, status: str, result: dict):
    """é€šçŸ¥ API Server ä»»å‹™ç‹€æ…‹å·²æ›´æ–°ï¼Œä»¥ä¾¿å»£æ’­çµ¦å‰ç«¯ã€‚"""
    try:
        # æ³¨æ„ï¼šé€™è£¡å‡è¨­ api_server åœ¨ 42649 port ä¸Šé‹è¡Œ (æ ¹æ“š circus.ini)
        notify_url = "http://127.0.0.1:42649/api/internal/notify_task_update"
        frontend_payload = {
            "task_id": task_id,
            "status": status,
            "result": result
        }
        requests.post(notify_url, json=frontend_payload, timeout=5)
        log.info(f"âœ… å·²æˆåŠŸç™¼é€ {status} é€šçŸ¥çµ¦ API Server: {task_id}")
    except requests.exceptions.RequestException as e:
        log.error(f"âŒ ç™¼é€ {status} é€šçŸ¥çµ¦ API Server å¤±æ•—: {e}")

def process_download_task(task: dict, use_mock: bool):
    """è™•ç†æ¨¡å‹ä¸‹è¼‰ä»»å‹™ã€‚"""
    task_id = task['task_id']
    payload = json.loads(task['payload'])
    model_size = payload['model_size']
    log.info(f"ğŸš€ é–‹å§‹è™•ç† 'download' ä»»å‹™: {task_id} for model '{model_size}'")

    # åœ¨æ¨¡æ“¬æ¨¡å¼ä¸‹ï¼Œæˆ‘å€‘ä¹Ÿå‡è£ä¸‹è¼‰
    if use_mock:
        log.info("(æ¨¡æ“¬) å‡è£ä¸‹è¼‰æ¨¡å‹...")
        time.sleep(3)
        db_client.update_task_status(task_id, 'completed', json.dumps({"message": "æ¨¡å‹å·²æˆåŠŸä¸‹è¼‰ (æ¨¡æ“¬)"}))
        return

    # çœŸå¯¦æ¨¡å¼ä¸‹ï¼Œå‘¼å«å·¥å…·çš„ download å‘½ä»¤
    tool_script_path = TOOLS_DIR / "transcriber.py"
    command = [sys.executable, str(tool_script_path), f"--command=download", f"--model_size={model_size}"]

    # æˆ‘å€‘å¯ä»¥åƒè½‰éŒ„ä¸€æ¨£ç›£è½é€²åº¦ï¼Œä½† download_model ç›®å‰åªåœ¨çµæŸæ™‚è¼¸å‡ºä¸€æ¬¡
    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, encoding='utf-8')

    for line in process.stdout:
        try:
            progress_data = json.loads(line)
            db_client.update_task_progress(task_id, progress_data.get("progress", 0), progress_data.get("log", ""))
        except json.JSONDecodeError:
            log.info(f"[ä¸‹è¼‰å·¥å…· stdout] {line.strip()}")

    process.wait()
    if process.returncode == 0:
        db_client.update_task_status(task_id, 'completed', json.dumps({"message": f"æ¨¡å‹ {model_size} å·²æˆåŠŸä¸‹è¼‰"}))
    else:
        log.error(f"âŒ ä¸‹è¼‰æ¨¡å‹ {model_size} å¤±æ•—ã€‚")
        db_client.update_task_status(task_id, 'failed', json.dumps({"error": f"ä¸‹è¼‰æ¨¡å‹ {model_size} å¤±æ•—"}))


def process_transcription_task(task: dict, use_mock: bool):
    """è™•ç†éŸ³è¨Šè½‰éŒ„ä»»å‹™ã€‚"""
    task_id = task['task_id']
    log.info(f"ğŸš€ é–‹å§‹è™•ç† 'transcribe' ä»»å‹™: {task_id}")
    try:
        payload = json.loads(task['payload'])
        input_file = Path(payload['input_file'])
        model_size = payload.get('model_size', 'tiny')
        language = payload.get('language')

        if not input_file.exists():
            raise FileNotFoundError(f"è¼¸å…¥æª”æ¡ˆä¸å­˜åœ¨: {input_file}")

        # 1. æ±ºå®šè¦ä½¿ç”¨çš„å·¥å…·
        tool_script_name = "mock_transcriber.py" if use_mock else "transcriber.py"
        tool_script_path = TOOLS_DIR / tool_script_name
        if not tool_script_path.exists():
             raise FileNotFoundError(f"å·¥å…·è…³æœ¬ä¸å­˜åœ¨: {tool_script_path}")

        # 2. æº–å‚™è¼¸å‡ºè·¯å¾‘
        TRANSCRIPTS_DIR.mkdir(exist_ok=True)
        output_file = TRANSCRIPTS_DIR / f"{task_id}.txt"

        # 3. æ§‹å»ºä¸¦åŸ·è¡Œå‘½ä»¤
        command = [
            sys.executable,
            str(tool_script_path),
            f"--command=transcribe", # æ˜ç¢ºæŒ‡å®šå‘½ä»¤
            f"--audio_file={input_file}",
            f"--output_file={output_file}",
            f"--model_size={model_size}"
        ]
        if language:
            command.append(f"--language={language}")

        log.info(f"ğŸ”§ åŸ·è¡Œå‘½ä»¤: {' '.join(command)}")

        # æ”¹ç”¨ Popen é€²è¡Œéé˜»å¡å¼è®€å–
        process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, encoding='utf-8')

        # 4. å³æ™‚è®€å– stdout ä¾†æ›´æ–°é€²åº¦
        full_stdout = []
        full_stderr = []

        # ä½¿ç”¨ç·’ä¾†é¿å…é˜»å¡
        def read_stderr():
            for line in process.stderr:
                full_stderr.append(line)
                log.warning(f"[å·¥å…· stderr] {line.strip()}")

        import threading
        stderr_thread = threading.Thread(target=read_stderr)
        stderr_thread.start()

        for line in process.stdout:
            full_stdout.append(line)
            try:
                # è§£æ JSON é€²åº¦
                progress_data = json.loads(line)
                progress = progress_data.get("progress")
                text = progress_data.get("text")
                if progress is not None:
                    log.info(f"ğŸ“ˆ ä»»å‹™ {task_id} é€²åº¦: {progress}% - {text[:30]}...")
                    db_client.update_task_progress(task_id, progress, text)
            except json.JSONDecodeError:
                # ä¸æ˜¯ JSON æ ¼å¼çš„æ—¥èªŒï¼Œç›´æ¥å°å‡º
                log.info(f"[å·¥å…· stdout] {line.strip()}")

        process.wait()
        stderr_thread.join()

        # 5. è™•ç†æœ€çµ‚çµæœ
        if process.returncode == 0:
            log.info(f"âœ… å·¥å…·æˆåŠŸå®Œæˆä»»å‹™: {task_id}")
            final_transcript = output_file.read_text(encoding='utf-8').strip()
            result_obj = {
                "transcript": final_transcript,
                "transcript_path": str(output_file), # æ–°å¢æ­¤è¡Œï¼Œç‚ºä¸‹è¼‰ API æä¾›è·¯å¾‘
                "tool_stdout": "".join(full_stdout),
            }
            db_client.update_task_status(task_id, 'completed', json.dumps(result_obj))
            log.info(f"âœ… ä»»å‹™ {task_id} ç‹€æ…‹å·²æ›´æ–°è‡³è³‡æ–™åº«ã€‚")

            # æ­¥é©Ÿ 6: é€šçŸ¥ API Server ä»»å‹™å·²å®Œæˆ
            notify_api_server(task_id, 'completed', result_obj)

        else:
            log.error(f"âŒ å·¥å…·åŸ·è¡Œä»»å‹™å¤±æ•—: {task_id}ã€‚è¿”å›ç¢¼: {process.returncode}")
            error_message = "".join(full_stderr) or "".join(full_stdout) or "æœªçŸ¥éŒ¯èª¤"
            result_obj = {
                "error": error_message,
                "tool_stdout": "".join(full_stdout),
                "tool_stderr": "".join(full_stderr)
            }
            db_client.update_task_status(task_id, 'failed', json.dumps(result_obj))
            notify_api_server(task_id, 'failed', result_obj)

    except Exception as e:
        log.critical(f"ğŸ’¥ è™•ç†ä»»å‹™ {task_id} æ™‚ç™¼ç”Ÿæœªé æœŸçš„åš´é‡éŒ¯èª¤: {e}", exc_info=True)
        result_obj = {"error": str(e)}
        db_client.update_task_status(task_id, 'failed', json.dumps(result_obj))
        notify_api_server(task_id, 'failed', result_obj)


def process_youtube_chain_task(task: dict, use_mock: bool):
    """
    è™•ç† YouTube ä¸‹è¼‰å’Œ AI åˆ†æä»»å‹™éˆã€‚
    é€™å°‡å–ä»£ api_server.py ä¸­çš„ thread-based æ–¹æ³•ã€‚
    """
    task_id = task['task_id']
    task_type = task.get('type')
    log.info(f"ğŸš€ é–‹å§‹è™•ç† '{task_type}' ä»»å‹™: {task_id}")

    # --- æ­¥é©Ÿ 1: æ±ºå®šå·¥ä½œç›®éŒ„ ---
    # å°æ–¼ gemini_process ä»»å‹™ï¼Œå®ƒé‡è¤‡ä½¿ç”¨å…¶çˆ¶ä»»å‹™çš„ç›®éŒ„
    parent_task_id = task.get('depends_on')
    work_dir_id = parent_task_id if parent_task_id else task_id
    work_dir = UPLOADS_DIR / work_dir_id
    work_dir.mkdir(exist_ok=True)
    log.info(f"ğŸ“ ä»»å‹™ {task_id} å°‡ä½¿ç”¨å·¥ä½œç›®éŒ„: {work_dir}")


    try:
        payload = json.loads(task['payload'])
        # --- æ­¥é©Ÿ 2: æ ¹æ“šä»»å‹™é¡å‹åŸ·è¡Œå°æ‡‰çš„å·¥å…· ---

        if task_type in ['youtube_download', 'youtube_download_only']:
            url = payload.get('url')
            if not url:
                raise ValueError("ä»»å‹™ payload ä¸­ç¼ºå°‘ 'url'")

            # æº–å‚™ downloader æŒ‡ä»¤
            tool_script = TOOLS_DIR / ("mock_youtube_downloader.py" if use_mock else "youtube_downloader.py")
            cmd = [
                sys.executable, str(tool_script),
                "--url", url,
                "--output-dir", str(work_dir), # ä½¿ç”¨éš”é›¢ç›®éŒ„
                "--download-type", payload.get("download_type", "audio")
            ]
            if payload.get("custom_filename"):
                cmd.extend(["--custom-filename", payload.get("custom_filename")])

            cookies_path = UPLOADS_DIR / "cookies.txt"
            if cookies_path.is_file():
                cmd.extend(["--cookies-file", str(cookies_path)])

            log.info(f"ğŸ”§ åŸ·è¡Œä¸‹è¼‰æŒ‡ä»¤: {' '.join(cmd)}")
            process = subprocess.run(cmd, capture_output=True, text=True, check=True, encoding='utf-8')

            result_data = json.loads(process.stdout)
            if result_data.get("status") == "failed":
                raise RuntimeError(result_data.get("error", "ä¸‹è¼‰å™¨å›å ±äº†ä¸€å€‹æœªçŸ¥çš„éŒ¯èª¤"))

            # å°‡çµæœä¸­çš„æª”æ¡ˆè·¯å¾‘è½‰æ›ç‚ºå¯å…¬é–‹å­˜å–çš„ URL
            for key in ["output_path", "html_report_path", "pdf_report_path"]:
                if key in result_data and result_data[key]:
                    result_data[key] = convert_to_media_url(result_data[key])

            db_client.update_task_status(task_id, 'completed', json.dumps(result_data))
            notify_api_server(task_id, 'completed', result_data)

        elif task_type == 'gemini_process':
            if not parent_task_id:
                raise ValueError("gemini_process ä»»å‹™å¿…é ˆä¾è³´æ–¼ä¸€å€‹ä¸‹è¼‰ä»»å‹™ ('depends_on' æ¬„ä½ç¼ºå¤±)")

            # ç²å–çˆ¶ä»»å‹™çš„çµæœ
            parent_task_info = db_client.get_task_status(parent_task_id)
            if not parent_task_info or parent_task_info.get('status') != 'completed':
                raise RuntimeError(f"çˆ¶ä»»å‹™ {parent_task_id} å°šæœªæˆåŠŸå®Œæˆï¼Œç„¡æ³•ç¹¼çºŒã€‚")

            parent_result = json.loads(parent_task_info.get('result', '{}'))
            media_url = parent_result.get('output_path')
            if not media_url:
                raise ValueError(f"çˆ¶ä»»å‹™ {parent_task_id} çš„çµæœä¸­æ‰¾ä¸åˆ° 'output_path'")

            media_file_path = convert_media_url_to_path(media_url)
            if not media_file_path.exists():
                raise FileNotFoundError(f"æ‰¾ä¸åˆ° Gemini åˆ†ææ‰€éœ€çš„åª’é«”æª”æ¡ˆ: {media_file_path}")

            # æº–å‚™ processor æŒ‡ä»¤
            tool_script = TOOLS_DIR / ("mock_gemini_processor.py" if use_mock else "gemini_processor.py")
            cmd = [
                sys.executable, str(tool_script),
                "--command", "process",
                "--audio-file", str(media_file_path),
                "--output-dir", str(work_dir), # å ±å‘Šä¹Ÿè¼¸å‡ºåˆ°åŒä¸€å€‹éš”é›¢ç›®éŒ„
                "--model", payload.get("model"),
                "--video-title", parent_result.get("video_title", "ç„¡æ¨™é¡Œ"),
                "--tasks", payload.get("tasks", "summary,transcript"),
                "--output-format", payload.get("output_format", "html")
            ]

            log.info(f"ğŸ”§ åŸ·è¡Œ Gemini åˆ†ææŒ‡ä»¤: {' '.join(cmd)}")
            # æ³¨æ„ï¼šgemini_processor.py å¯èƒ½æœƒå°‡é€²åº¦æ›´æ–°å¯«å…¥ stderr
            process = subprocess.run(cmd, capture_output=True, text=True, check=True, encoding='utf-8')

            result_data = json.loads(process.stdout)
            if result_data.get("status") == "failed":
                raise RuntimeError(result_data.get("error", "Gemini åˆ†æå™¨å›å ±äº†ä¸€å€‹æœªçŸ¥çš„éŒ¯èª¤"))

            # å°‡çµæœä¸­çš„æª”æ¡ˆè·¯å¾‘è½‰æ›ç‚ºå¯å…¬é–‹å­˜å–çš„ URL
            for key in ["output_path", "html_report_path", "pdf_report_path"]:
                if key in result_data and result_data[key]:
                    result_data[key] = convert_to_media_url(result_data[key])

            db_client.update_task_status(task_id, 'completed', json.dumps(result_data))
            notify_api_server(task_id, 'completed', result_data)

        else:
            raise ValueError(f"åœ¨ process_youtube_chain_task ä¸­é‡åˆ°æœªçŸ¥çš„ä»»å‹™é¡å‹: {task_type}")

    except (subprocess.CalledProcessError, RuntimeError, ValueError, FileNotFoundError) as e:
        log.error(f"âŒ è™•ç†ä»»å‹™ {task_id} ({task_type}) å¤±æ•—: {e}")
        error_message = str(e)
        if isinstance(e, subprocess.CalledProcessError):
            error_message = e.stderr or "yt-dlp åŸ·è¡Œå¤±æ•—ä¸”æœªæä¾› stderrã€‚"

        # å˜—è©¦å¾éŒ¯èª¤è¨Šæ¯ä¸­è§£æ JSON
        try:
            error_payload = json.loads(error_message)
        except json.JSONDecodeError:
            error_payload = {"error": error_message}

        db_client.update_task_status(task_id, 'failed', json.dumps(error_payload))
        notify_api_server(task_id, 'failed', error_payload)

    except Exception as e:
        log.critical(f"ğŸ’¥ è™•ç†ä»»å‹™ {task_id} æ™‚ç™¼ç”Ÿæœªé æœŸçš„åš´é‡éŒ¯èª¤: {e}", exc_info=True)
        error_payload = {"error": f"Worker å…§éƒ¨åš´é‡éŒ¯èª¤: {e}"}
        db_client.update_task_status(task_id, 'failed', json.dumps(error_payload))
        notify_api_server(task_id, 'failed', error_payload)

    finally:
        # --- æ­¥é©Ÿ 3: æ¸…ç†éš”é›¢ç›®éŒ„ ---
        # æ¸…ç†åªæ‡‰åœ¨ä»»å‹™éˆçš„æœ€å¾Œä¸€å€‹ä»»å‹™å®Œæˆå¾Œé€²è¡Œã€‚
        task_is_final_in_chain = task_type in ['youtube_download_only', 'gemini_process']

        if task_is_final_in_chain:
            # work_dir æ˜¯æ ¹æ“š parent_task_id æˆ– task_id æ±ºå®šçš„ï¼Œæ‰€ä»¥è·¯å¾‘æ˜¯æ­£ç¢ºçš„
            if work_dir.exists():
                try:
                    shutil.rmtree(work_dir)
                    log.info(f"ğŸ—‘ï¸ å·²æˆåŠŸæ¸…ç†ä»»å‹™éˆçš„éš”é›¢ç›®éŒ„: {work_dir}")
                except Exception as e:
                    log.error(f"æ¸…ç†éš”é›¢ç›®éŒ„ {work_dir} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
            else:
                log.warning(f"æƒ³è¦æ¸…ç†çš„ç›®éŒ„ {work_dir} ä¸å­˜åœ¨ï¼Œå¯èƒ½å·²è¢«æå‰æ¸…ç†ã€‚")
        else:
            log.info(f"â„¹ï¸ ä»»å‹™ {task_id} ({task_type}) ä¸æ˜¯éˆçš„çµ‚é»ï¼Œè·³éæ¸…ç†æ­¥é©Ÿã€‚")


def process_task(task: dict, use_mock: bool):
    """
    æ ¹æ“šä»»å‹™é¡å‹åˆ†æ´¾åˆ°ä¸åŒçš„è™•ç†å‡½å¼ã€‚
    """
    task_type = task.get('type', 'transcribe') # é è¨­ç‚ºèˆŠçš„è½‰éŒ„ä»»å‹™
    if task_type == 'download':
        process_download_task(task, use_mock)
    elif task_type == 'transcribe':
        process_transcription_task(task, use_mock)
    elif task_type in ['youtube_download', 'youtube_download_only', 'gemini_process']:
        process_youtube_chain_task(task, use_mock)
    else:
        log.error(f"âŒ æœªçŸ¥çš„ä»»å‹™é¡å‹: '{task_type}' (Task ID: {task['task_id']})")
        db_client.update_task_status(task['task_id'], 'failed', json.dumps({"error": f"æœªçŸ¥çš„ä»»å‹™é¡å‹: {task_type}"}))

def main_loop(use_mock: bool, poll_interval: int):
    """
    å·¥äººçš„ä¸»è¿´åœˆï¼ŒæŒçºŒå¾ä½‡åˆ—ä¸­æ‹‰å–ä¸¦è™•ç†ä»»å‹™ã€‚
    """
    log.info(f"ğŸ¤– Worker å·²å•Ÿå‹•ã€‚æ¨¡å¼: {'æ¨¡æ“¬ (Mock)' if use_mock else 'çœŸå¯¦ (Real)'}ã€‚æŸ¥è©¢é–“éš”: {poll_interval} ç§’ã€‚")
    try:
        while True:
            task = db_client.fetch_and_lock_task()
            if task:
                process_task(task, use_mock)
            else:
                # ä½‡åˆ—ç‚ºç©ºï¼Œç¨ä½œç­‰å¾…
                time.sleep(poll_interval)
    except KeyboardInterrupt:
        log.info("ğŸ›‘ æ”¶åˆ°ä¸­æ–·ä¿¡è™Ÿï¼ŒWorker æ­£åœ¨é—œé–‰...")
    except Exception as e:
        log.critical(f"ğŸ”¥ Worker ä¸»è¿´åœˆç™¼ç”Ÿè‡´å‘½éŒ¯èª¤: {e}", exc_info=True)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="èƒŒæ™¯å·¥ä½œè™•ç†å™¨ã€‚")
    parser.add_argument(
        "--mock",
        action="store_true",
        help="å¦‚æœè¨­ç½®æ­¤æ——æ¨™ï¼Œå‰‡ä½¿ç”¨ mock_transcriber.py é€²è¡Œæ¸¬è©¦ã€‚"
    )
    parser.add_argument(
        "--poll-interval",
        type=int,
        default=5,
        help="ç•¶ä½‡åˆ—ç‚ºç©ºæ™‚ï¼Œè¼ªè©¢è³‡æ–™åº«çš„é–“éš”æ™‚é–“ï¼ˆç§’ï¼‰ã€‚"
    )
    args = parser.parse_args()

    # JULES'S REFACTOR: The worker no longer initializes the database directly.
    # The db_manager service is responsible for this.
    # database.initialize_database()

    # ç„¶å¾Œè¨­å®šæ—¥èªŒ
    setup_database_logging()

    main_loop(args.mock, args.poll_interval)
